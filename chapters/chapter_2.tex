
MFCC and Chroma STFT \parencite{1} stack together features which are used in LTSM model are giving state-of-the-art result. In our world we are surrounded with many sounds which is not possible to process all by our brain cell. Thats why many research field are work on sound classification for many purposes such as audio surveillance, multimedia and many more. so, as said before there are many features that are used in this paper. Those are mel spectrogram, spectral contrast, tonnetz, MFCC, Chroma CENS and Chroma CQT. From models there are used CNN and LSTM.There are lots of other papers that are used in this paper for better performance. In dataset is shows deep learning models are performing better performance then machine learning models. The audio samples were in wave from but it was converted into a one-dimensional NumPy array of digital values. There was a librosa package used for normalizing so that values can be represented in the array. Also works as time stretch and pitch shifting. After implementing all spectrograms in CNN and LSTM it shows MFCC was best features. Where stacking MFCC and Chroma STFT helps to reach validation accuracy of 98.81\% where best performance was 98.60\%. Also, LTSM does a better job because LSTM memory cell includes constant error backpropagation which deals with data noise. 

Cardiovascular diseases have leading cause of mortality rate where, Wavelet representations and RNN \parencite{2} can be used for recognizing three heart sounds i.e., normal, mild, severe. This works needs physicians who are trained also approx. 20\%of the medical interns on average can use efficient use of stereoscope to measure subjectsâ€™ heartbeat. There are some limitations in work such as publicly accessible data, deep learning methods are not 8 comprehensively studied. In the experiment coif3 was selected for wavelet type. DRNN model was optimized to have three layers with adam optimizer. So, in results wavelet based DRNN works excellent performance in reckoning mild class. The value of normal and severe can be improved. This is hard for this model to distinguish between these three models. 

Machine learning advances have sparked renewed interest \parencite{3} in a variety of classification challenges, particularly those incorporating data in the form of photos, videos, and audio recordings. Classifying sounds and predicting their category is one of the most common classification challenges. Security systems, classifying music clips to identify the genre of the song, classifying distinct surrounding sounds, speaker recognition, and verification are some of the real-world uses for such a classification model. The task of assessing diverse audio signals is known as audio classification. They gave a brief overview of the field of audio classification in this work, describing the system, several modules of feature extraction and modeling, applications, underlying approaches, and certain performance indicators. Following this introduction, we'll go over some of the present classification technologies' strengths and drawbacks, as well as some prospective future research, development, and application trends. Many audio classification subtasks rely heavily on inputs, network topologies, temporal pooling strategies, and objective functions, so we paid special attention to them. Finally, the study discusses future trends and research prospects in this field.

A novel audio finger methodology  for audio classification is proposed in this research. The audio signal's fingerprint is a unique digest that can be used to identify it. To establish a unique fingerprint of the audio files, the suggested model employs the audio fingerprinting methodology. The fingerprints are made by 9 extracting an MFCC spectrum, taking the mean of the spectra, and then converting the spectrum to a binary picture. These images are then sent into the LSTM network, which classifies the environmental noises included in the UrbanSound8K dataset with an accuracy of 98.8 \% across all 10 folds. 

It is critical to provide security for women and children in light of increased crime against them. Several contemporary techniques Nowadays, they're employed to provide security. Some of them are sensor-based gadgets \parencite{5}, while others are mobile apps. But When sensors are present, hardware-based gadgets do not function properly and are cut off from their bodies Existing applications on the Mobile gadgets are not always functional. Victims must take action. Specific reactions such as shaking the phone and pressing the SOS button, which may or may not be available at all times. As audio one of the most advanced applications of deep learning is classification. They presented an idea to provide protection for women and children through learning. Audio classification is used by youngsters. The victim's screaming sounds, which can be heard from quite a distance, have alerted them of danger. For audio classification, a few deep neural network models are used, so various audio and reaction screams can be considered a sign of danger. They've also created a new dataset specifically for this. 

A novel strategy for dealing with the presence of unseen sound classes (open set) and the scarcity of training resources \parencite{6} while deploying audio classification systems in operations is proposed, which incorporates variational auto-encoder (VAE), data augmentation, and detection-classification combined training into standard GAN networks. The VAE input to GAN-generator aids in the generation of realistic outlier samples that are not too distant from the in-distribution class, hence improving classifiers' open-set discriminating capabilities. The augmentation enhanced GAN scheme developed in their previous work for close-set audio classification will then help to address the limited training resources by combining physical data 10 augmentation with traditional GAN produced samples to avoid overfitting and improve optimization convergences. The detection-classification joint training builds on the advantages of VAE and Augmentation GAN to improve detection and classification task performance. Experiments using the Google Speech Command database indicate significant increases in open set classification accuracy from 62.41 percent to 88.29 percent when only 10\% of the training data is used. 

A Deep Belief Network (DBN)-based technique \parencite{7} for audio signal classification is proposed to improve labor activity identification and remote construction project surveillance. The goal of this project is to provide an accurate and adaptable platform for performing and controlling unmanned construction site monitoring utilizing distributed sound sensors. To perform and validate the proposed approach, 10 classes of numerous construction equipment and tools, often and widely used on construction sites, were collected and examined in this study. The DBN receives a concatenation of multiple statistics evaluated by a set of spectral features, such as MFCCs and mel-scaled spectrograms. The proposed architecture, as well as the preprocessing and feature extraction stages, have been thoroughly defined, and the usefulness of the proposed concept has been shown by numerical results based on real-world recordings. The final total accuracy on the test set was up to 98 percent, which is much better than other state-of-the-art methodologies. In order to apply the categorization scheme to sound data acquired in various environmental settings, a practical and real-time application of the presented method has also been proposed. 

Sound files for Animal sound classification using deep learning and CNN architecture, were preprocessed \parencite{8} in such a way that it can extract MFCC using librosa. There are also some studies that show that with a limited dataset CNN using log MEL-spectrogram performs best with 64.5 accuracy. 11 The data was collected online in WAV format and these are used in 3 different ways. First, they examine all manually to prevent low quality sound. Secondly all are WAV format because MFCC supports WAV format in feature extraction. Thirdly, the size of the sound file was 3 kilobytes. By using Librosa all dataset were preprocessed as binary files for Training and Testing. For activation functions it used Relu. For testing it takes 80\% of data and for training it takes 20\% data and finally it shows 75\% accuracy by Nesterov-accelerated adaptive moment estimation. 

Urban sound classifications have multiple features, which are implicated on different neural networks to see which model gives better accuracy in classifying audio signals. In our world we are surrounded with many sounds which are not possible to process all by our brain cells. That's why many research fields work on sound classification for many purposes such as audio surveillance, multimedia and many more. so, as said before, there are many features that are used in this paper. Those are mel spectrogram, spectral contrast,tonnetz,MFCC,Chroma CENS and Chroma CQT. From models there are CNN and LSTM.There are lots of other papers that are used in this paper for better performance. In dataset it shows deep learning models are performing better performance then machine learning models. The audio samples were in wave form but they were converted into a one-dimensional NumPy array of digital values. There was a librosa package used for normalizing so that values can be represented in the array. Also works as time stretch and pitch shifting. After implementing all spectrograms in CNN and LSTM it shows MFCC has the best features. Where stacking MFCC and Chroma STFT helps to reach validation accuracy of 98.81\% where best performance was 98.60\%. Also, LTSM does a better job because LTSM memory cell includes constant error back propagation which deals with data noise. So, lastly this paper shows that using MFCC and Chroma STFT stack together features which are used in the LTSM model are giving state-of-the-art results.

CNN is a good way to implement in Environmental sound classifications \parencite{12}. For this CNN there are some steps that were followed in this paper. All the data are labeled for efficient learning. There were multiple datasets used (ECS-50, ECS-10). After implementing it in CNN it shows better results even with a limited dataset. When the dataset number will increase it will perform more accurately in the environmental dataset. Speaking Faces is made up of synchronized audio, thermal, and visual data collected from a wide range of people. They used their data to do thermal-to-visible image translation and multimodal gender classification using thermal, visual, and auditory data streams to illustrate its applicability. They saw that Speaking Faces has the following good effects based on the experimental results. For starters, it allows for more in-depth research into multimodal recognition systems that use optical, thermal, and aural modalities. Second, the enormous number of samples in the dataset makes it possible to build and test data-hungry neural network techniques. Finally, synced multimodal data can offer up new avenues for domain transfer study. They intend to use our dataset for more multimodal tasks in the future, such as audioâ€“visualâ€“thermal speech and speaker recognition. By leveraging 150 hours of OOD data and adjusting with 5 hours of in-domain transcribed data, SR models for controller pilot communication for the Vienna approach were developed. To take use of inexpensively available untranscribed in-domain data, we suggested data selection algorithms based on word and concept level confidences. This was used to supplement transcribed in-domain data, allowing both acoustic and language models to be adjusted. When compared to using only in-domain transcribed data (ASR- DEV1, WER: 12.3 percent, CER: 38.6 percent), using OOD data and complementing transcribed data with untranscribed in-domain data through data selection reduces WER by 23.5 \% (using word confidences) and CER by 7\% (using concept confidences). We will investigate using larger volumes of untranscribed data in the future. 

A hybrid classical-to-quantum transfer learning method \parencite{13} for QNNs can be used in SCR where, CNN-QNN-based SCR system after setting up the VQC-based QNN. A pre-trained CNN framework is transferred to a hybrid transfer learning framework.so that we might improve the performance of CNN-QNN system Using the Google speech command dataset, found that, the importance of hybrid classical-to-quantum transfer learning in improving classification precision and reducing cross-entropy loss the CNN-QNN model's value. 
A smart home system \parencite{10} saves time and energy, especially when a large number of people are involved. A large number of people are involved. It might be expanded to include video surveillance to detect persons in crowded places like bus stops, theaters, and train stations, where the perpetrator's identity can be verified. Face recognition techniques are used. In the field of computer vision, the recognition system is a challenging matter to tackle. Due to its wide uses in a variety of sectors, it has recently sparked a lot of attention. Despite extensive efforts, Research efforts in this field have yielded robust facial recognition systems that can work in a variety of environments. They are still far from fulfilling the ideal of being able to perform well in limited spaces. 

Door Access Control System \parencite{11} only enables those who have an approved key card and whose voice has been recorded in the system. The technology of voice recognition recognizes your voice and the words you say. Because your voice cannot be stolen, it is a safe way to prevent illicit admission into an organization. If you're experiencing difficulty speaking, An RFID-enabled key card can be utilized (probably sick). As a prototype, the work was successful, and the prototype was successful. 14 The door was able to unlock when the user spoke into the microphone or swiped the RFID card. This is the first version. Further improvements to this work can be done before. 

Adversarial attacks \parencite{12} are the biggest threat to speech recognition, especially for Deep Learning-based systems. Some researchers have created adversarial speech examples for speech recognition based on FGSM. Genetic algorithms have also been used to fool speech recognition systems into saying that all is well. In this paper, they investigate the effectiveness of adversarial attacks on speech recognition components for mission-critical applications and provide defense techniques against adversarial attacks. They also outline the challenges and directions for future research as well as outline the current state of the art in speech recognition. Speech recognition provides a natural interface for human communication. It allows machines to process human voice and to generate a text transcription. Integrating voice recognition with conversational systems opens the door for incredible potential in many real-world applications. There are different methods for feature extraction such as Mel Frequency Cepstral Coefficients (MFCC), Perceptual Linear Prediction (PLP), linear predictive coding (LPC), Discrete wavelet transform (DWT), Linear prediction cepstral coefficient (LPCC), Fast Fourier Transform (FFT), and Line spectral frequencies (LSF). The aim of the decoding step is to convert an input audio into words by searching the most likely sequence. This task is mainly carried out by learning algorithms such as Hidden Markov Models (HMMs using the Viterbi algorithm that limits the number of searches and finds the optimal path in polynomial time) or deep learning. GMM-HMM is a statistical approach to estimate hidden information from visual signals. The decoder in a speech recognition system is modeled as a Markov process with unknown parameters, signified by the known observable parameters. Integrating a Gaussian Mixture Model with Hidden Markov Model outperformed conventional HMM and GMM-based systems. Hybrid DNN-HMM models are the earliest Deep Learning approaches for speech recognition, where a DNN replaces the acoustic module while keeping the remaining modules. The current trend is to build end-to-end speech recognition systems based on DNNs. This approach can resolve the limitation of the traditional system in which the overall system may not be optimal.

\parencite{14} Features are initially extracted from short-time Fourier transform (STFT) spectrograms via a convolutional neural network. The model was trained and tested on the ICBHI 2017 respiratory 16 Sound Database and achieved state-of-the-art results using three different data splitting strategies. Results include sensitivity 47.37, specificity 82.46\%, score 64.92\% and accuracy 73.69\%. Features initially extracted from short-time Fourier transform (STFT) spectrograms via a convolutional neural network (CNN) are given as input to a long short-term memory (LSTM) network that memorizes the temporal dependencies between data and classifies four types of lung sounds, including normal, crackles, wheezes, and both crackles and wheezes. In this paper they proposed a hybrid DL architecture that combines CNN and LSTM. models for the classification of normal and adventitious lung sounds. Training and evaluation of the model are performed on lung sounds, which includes normal sounds as well as three types of adventitious sounds such as crackles and wheezes. In this study, respiratory cycles were transformed into STFT spectrogram\r images and passed through a CNN that is capable of extracting their most significant features. The extracted features were fed into an LSTM that identifies and memorizes long-term dependencies between them.
 
CNN-TT-DNN \parencite{15} model replaces fully connected (FC) layers with TT ones and can substantially reduce the number of model parameters while maintaining the baseline performance of the CNN model. Our experimental results show that the proposed CNN +(TT-DNN) model attains a competitive accuracy of 96.31\% with 4 times fewer model parameters than the CNN model. The entire CNN framework consists of 4 components, each of which is constructed by stacking 1D convolutional layers with batch normalization and the ReLU activation. The spectral features associated with the outputs of the CNN framework are fed into the FC layers or TT layers. We set our baseline system as the CNN+DNN architecture in which several FC layers are stacked on top of the CNN layers. THEIR models are trained with the same SCR dataset from scratch without any data augmentation to make a fair architecture-wise study. We extend the 10 classes training setup used in 35 classes to report its final results. The CNN+(TT) and CNN+(DNN) models are compared with three other neural networks available in the literature. The CNN+DNN model is taken as the baseline SCR system, and attains 94.42 accuracy and 0.251 CE score. It outperforms the DenseNet, neural attention, and QCNN models in terms of smaller model size, lower CE value, and higher classification accuracy. 

TTTD \parencite{16} can maintain and even obtain better results than the primitive models. Baseline models (DenseNet, neural attention, and QCNN models), when the CNN+(TT-DNN) model is initialized randomly or derived from a well-trained CNN+DNN. Tucker decomposition cannot maintain the DNN baseline results as CNN +DNN models. This work investigates the implementation of a low-complexity SCR system by applying the TT technique. A hybrid model, namely, CNN+(TT-DNN), is proposed to realize an end-to-end SCR pipeline. The proposed model can be randomly initiated or derived from a well-trained CNN \+ DNN.

By taking into account trade-offs between the model complexity and actual performance, low complexity hybrid tensor networks [13] are created. Additionally, CNN+(LR-TT-DNN) has many TT layers at the top for problem-solving in regression and classification and convolutional layers at the bottom for feature extraction. First, we use a low-rank tensor-train deep neural network (TT-DNN) to create the LR-TTDNN, a whole deep learning pipeline. The optimization landscape is made simpler by an over-parameterized deep neural network (DNN), which also assures that local optimum points are near the global ones. Numerous novel applications, such as mobile-based audio de-noise and speech recognition systems that operate on users' phones without sending queries to a distant server where a sizable deep learning model is set up, might be made possible by an effective low-complexity speech improvement system. Two methods exist for reducing the size of deep learning models. One involves the adoption of new deep learning architectures, such as convolutional neural network (CNN) with a variety of innovative topologies. Model pruning and sparseness approaches are mentioned in another. The experiments of voice enhancement and spoken command recognition (SCR) systems are shown to demonstrate the usefulness of our suggested models as we study the deployment of low-rank tensor-train (TT) networks. They have created the CNN+TT-DNN, a brand-new deep hybrid tensor-train model. A CNN is at the bottom, and a DNN is at the top. Time-series signals are transformed into the relevant spectral properties using the CNN model. The DNN is used to address regression or classification issues further. To evaluate the empirical performance of the two models, we independently use spoken-recording experiments and speech enhancement. The TT-DNN model obtains a worse MAE score than the DNN model despite having significantly fewer model parameters (0.604Mb vs. 30.425Mb) (0.664 vs. 0.675). With greater PESQ and STOI scores, the hybrid model CNN+DNN may greatly increase the DNN basis. In this study, two brand-new TT modelsâ€”LR-TT DNN and CNN-TT-DNNâ€”were introduced, and their effectiveness was evaluated using SCR and voice enhancement tasks.
